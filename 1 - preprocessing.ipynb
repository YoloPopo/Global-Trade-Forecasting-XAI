{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-part1",
   "metadata": {},
   "source": [
    "# Part 1: Data Acquisition, Multi-Year Consolidation, and Initial Processing\n",
    "\n",
    "**Objective:** Create a clean, consolidated, multi-year dataset from raw annual trade data. This involves:\n",
    "1.  **Loading and Merging:** Combining individual yearly CSV files (1988-2024) from the `raw_import_data/` folder into a single DataFrame. The initial merge resulted in 795,683 records.\n",
    "2.  **Data Cleaning and Schema Standardization:** Extracting the 'Year' from the `refPeriodId` column, renaming key columns to a consistent schema (`importer`, `exporter`, `amount`, `year`), and filtering out non-country 'World' entries.\n",
    "3.  **Geographic Feature Enrichment:** Augmenting the data by adding latitude and longitude coordinates for both importer and exporter countries from a lookup file (`country-coord.csv`). Records with missing coordinates were dropped.\n",
    "4.  **Significance Filtering:** Applying a \"Top 20\" filter to retain only the most significant trade partners for each importer on an annual basis, focusing the analysis on the most impactful trade relationships.\n",
    "5.  **Final Output Generation:** Saving the final processed and filtered dataset, which contains 17,170 observations, to `trade_data.csv`.\n",
    "6.  **Workspace Cleanup:** Removing all intermediate temporary files generated during the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-setup-md",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup\n",
    "Import necessary libraries and define configuration variables for file paths, folder locations, and processing parameters like 'Top N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re # For robust year extraction from filenames\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_FOLDER = 'raw_import_data/' # Folder containing yearly CSV files (e.g., 2023.csv)\n",
    "COORDS_FILE = 'country-coord.csv'    # File with country coordinates\n",
    "TOP_N_PARTNERS = 20                  # Number of top partners to keep for each importer per year\n",
    "\n",
    "# Define temporary and final output filenames\n",
    "TEMP_MERGED_CLEANED_FILE = 'temp_merged_cleaned_data.csv'\n",
    "TEMP_WITH_COORDS_FILE = 'temp_data_with_coords.csv'\n",
    "FINAL_OUTPUT_FILE_PART1 = 'trade_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-merge-md",
   "metadata": {},
   "source": [
    "## 2. Load and Merge Yearly Data Files\n",
    "This step iterates through all `.csv` files located in the `RAW_DATA_FOLDER`. For each file, the year is parsed from the filename (e.g., '1988.csv' -> 1988). The data from each year is loaded into a pandas DataFrame and then concatenated into a single `master_df`. The output confirms that data from 1988 to 2024 was successfully loaded, resulting in a master DataFrame with 795,683 rows and 48 columns before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-merge-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 1988.csv for year 1988\n",
      "Processed: 1989.csv for year 1989\n",
      "Processed: 1990.csv for year 1990\n",
      "Processed: 1991.csv for year 1991\n",
      "Processed: 1992.csv for year 1992\n",
      "Processed: 1993.csv for year 1993\n",
      "Processed: 1994.csv for year 1994\n",
      "Processed: 1995.csv for year 1995\n",
      "Processed: 1996.csv for year 1996\n",
      "Processed: 1997.csv for year 1997\n",
      "Processed: 1998.csv for year 1998\n",
      "Processed: 1999.csv for year 1999\n",
      "Processed: 2000.csv for year 2000\n",
      "Processed: 2001.csv for year 2001\n",
      "Processed: 2002.csv for year 2002\n",
      "Processed: 2003.csv for year 2003\n",
      "Processed: 2004.csv for year 2004\n",
      "Processed: 2005.csv for year 2005\n",
      "Processed: 2006.csv for year 2006\n",
      "Processed: 2007.csv for year 2007\n",
      "Processed: 2008.csv for year 2008\n",
      "Processed: 2009.csv for year 2009\n",
      "Processed: 2010.csv for year 2010\n",
      "Processed: 2011.csv for year 2011\n",
      "Processed: 2012.csv for year 2012\n",
      "Processed: 2013.csv for year 2013\n",
      "Processed: 2014.csv for year 2014\n",
      "Processed: 2015.csv for year 2015\n",
      "Processed: 2016.csv for year 2016\n",
      "Processed: 2017.csv for year 2017\n",
      "Processed: 2018.csv for year 2018\n",
      "Processed: 2019.csv for year 2019\n",
      "Processed: 2020.csv for year 2020\n",
      "Processed: 2021.csv for year 2021\n",
      "Processed: 2022.csv for year 2022\n",
      "Processed: 2023.csv for year 2023\n",
      "Processed: 2024.csv for year 2024\n",
      "\n",
      "Master DataFrame shape after merging: (795683, 48)\n",
      "Master DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 795683 entries, 0 to 795682\n",
      "Data columns (total 48 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   typeCode                  795683 non-null  object \n",
      " 1   freqCode                  795683 non-null  int64  \n",
      " 2   refPeriodId               795683 non-null  int64  \n",
      " 3   refYear                   795683 non-null  int64  \n",
      " 4   refMonth                  795683 non-null  int64  \n",
      " 5   period                    795683 non-null  int64  \n",
      " 6   reporterCode              795683 non-null  object \n",
      " 7   reporterISO               795683 non-null  object \n",
      " 8   reporterDesc              795683 non-null  object \n",
      " 9   flowCode                  795683 non-null  object \n",
      " 10  flowDesc                  795683 non-null  int64  \n",
      " 11  partnerCode               795683 non-null  object \n",
      " 12  partnerISO                795683 non-null  object \n",
      " 13  partnerDesc               795683 non-null  int64  \n",
      " 14  partner2Code              795683 non-null  object \n",
      " 15  partner2ISO               795683 non-null  object \n",
      " 16  partner2Desc              795683 non-null  object \n",
      " 17  classificationCode        795683 non-null  object \n",
      " 18  classificationSearchCode  795683 non-null  bool   \n",
      " 19  isOriginalClassification  795683 non-null  object \n",
      " 20  cmdCode                   795683 non-null  object \n",
      " 21  cmdDesc                   795683 non-null  int64  \n",
      " 22  aggrLevel                 795683 non-null  bool   \n",
      " 23  isLeaf                    795683 non-null  object \n",
      " 24  customsCode               795683 non-null  object \n",
      " 25  customsDesc               795683 non-null  int64  \n",
      " 26  mosCode                   795683 non-null  int64  \n",
      " 27  motCode                   795683 non-null  object \n",
      " 28  motDesc                   795683 non-null  int64  \n",
      " 29  qtyUnitCode               0 non-null       float64\n",
      " 30  qtyUnitAbbr               340599 non-null  float64\n",
      " 31  qty                       795683 non-null  bool   \n",
      " 32  isQtyEstimated            795683 non-null  int64  \n",
      " 33  altQtyUnitCode            0 non-null       float64\n",
      " 34  altQtyUnitAbbr            335513 non-null  float64\n",
      " 35  altQty                    795683 non-null  bool   \n",
      " 36  isAltQtyEstimated         286029 non-null  float64\n",
      " 37  netWgt                    795683 non-null  bool   \n",
      " 38  isNetWgtEstimated         292478 non-null  float64\n",
      " 39  grossWgt                  795683 non-null  bool   \n",
      " 40  isGrossWgtEstimated       759272 non-null  float64\n",
      " 41  cifvalue                  183748 non-null  float64\n",
      " 42  fobvalue                  795683 non-null  float64\n",
      " 43  primaryValue              795683 non-null  int64  \n",
      " 44  legacyEstimationFlag      795683 non-null  bool   \n",
      " 45  isReported                795683 non-null  bool   \n",
      " 46  isAggregate               0 non-null       float64\n",
      " 47  Year                      795683 non-null  int64  \n",
      "dtypes: bool(8), float64(10), int64(14), object(16)\n",
      "memory usage: 248.9+ MB\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(os.path.join(RAW_DATA_FOLDER, \"*.csv\"))\n",
    "list_of_dfs = []\n",
    "\n",
    "if not all_files:\n",
    "    print(f\"Warning: No CSV files found in {RAW_DATA_FOLDER}. Please check the path.\")\n",
    "\n",
    "for f in all_files:\n",
    "    try:\n",
    "        filename = os.path.basename(f)\n",
    "        # Attempt to extract a 4-digit year from the filename robustly\n",
    "        year_str = filename.split('.')[0] # Initial attempt\n",
    "        if not year_str.isdigit() or len(year_str) != 4:\n",
    "            match = re.search(r'(\\d{4})', filename) # Regex fallback\n",
    "            if match:\n",
    "                year_str = match.group(1)\n",
    "            else:\n",
    "                # If year cannot be determined, it might be an issue or the column 'refPeriodId' is relied upon solely\n",
    "                raise ValueError(f\"Could not extract 4-digit year from filename: {filename}\")\n",
    "        year_from_filename = int(year_str)\n",
    "        \n",
    "        df_temp = pd.read_csv(f, encoding=\"latin1\", low_memory=False)\n",
    "        df_temp['Year'] = year_from_filename # This 'Year' is from filename\n",
    "        list_of_dfs.append(df_temp)\n",
    "        print(f\"Processed: {filename} for year {year_from_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {e}\")\n",
    "\n",
    "if not list_of_dfs:\n",
    "    raise FileNotFoundError(f\"No CSV files found or successfully processed in {RAW_DATA_FOLDER}. Project cannot continue.\")\n",
    "\n",
    "master_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nMaster DataFrame shape after merging: {master_df.shape}\")\n",
    "print(\"Master DataFrame Info:\")\n",
    "master_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-clean-md",
   "metadata": {},
   "source": [
    "## 3. Initial Cleaning, Renaming, and Filtering\n",
    "The `master_df` is subsetted to retain only the essential columns for the analysis: `reporterISO`, `partnerISO`, `cifvalue`, and `refPeriodId`. These columns are then renamed to a more intuitive and standardized schema: `importer`, `exporter`, `amount`, and `year`, respectively. The `year` column is derived from the `refPeriodId` in the raw data, making it the authoritative source for the time period. Any rows where the importer or exporter is listed as 'World' are removed, and records with non-numeric or missing trade `amount` values are dropped. The resulting cleaned DataFrame is then saved to a temporary file for the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial-clean-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned DataFrame head (before saving to temp file):\n",
      "    importer     exporter       amount  year\n",
      "1  Australia  Afghanistan     237515.0  1988\n",
      "2  Australia      Albania       4536.0  1988\n",
      "3  Australia      Algeria      96230.0  1988\n",
      "4  Australia    Argentina   41170679.0  1988\n",
      "5  Australia      Austria  114447116.0  1988\n"
     ]
    }
   ],
   "source": [
    "# Ensure required columns for renaming exist in master_df\n",
    "required_rename_cols = {'reporterISO', 'partnerISO', 'cifvalue', 'refPeriodId'}\n",
    "if not required_rename_cols.issubset(master_df.columns):\n",
    "    missing = required_rename_cols - set(master_df.columns)\n",
    "    raise KeyError(f\"One or more required columns for renaming are missing from master_df: {missing}. Available columns: {master_df.columns.tolist()}\")\n",
    "\n",
    "df_cleaned = master_df[['reporterISO', 'partnerISO', 'cifvalue', 'refPeriodId']].rename(\n",
    "    columns={\n",
    "        'reporterISO': 'importer',\n",
    "        'partnerISO': 'exporter',\n",
    "        'cifvalue': 'amount',\n",
    "        'refPeriodId': 'year' # 'refPeriodId' from data is now the primary 'year'\n",
    "    }\n",
    ").copy() # Use .copy() to ensure it's a new DataFrame\n",
    "\n",
    "# Convert 'amount' to numeric, errors will be coerced to NaN\n",
    "df_cleaned['amount'] = pd.to_numeric(df_cleaned['amount'], errors='coerce')\n",
    "# Remove rows where 'amount' became NaN after conversion or was originally NaN\n",
    "df_cleaned.dropna(subset=['amount'], inplace=True)\n",
    "\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned['importer'] != 'World') &\n",
    "    (df_cleaned['exporter'] != 'World')\n",
    "]\n",
    "\n",
    "print(f\"\\nCleaned DataFrame head (before saving to temp file):\")\n",
    "print(df_cleaned.head())\n",
    "df_cleaned.to_csv(TEMP_MERGED_CLEANED_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add-coords-md",
   "metadata": {},
   "source": [
    "## 4. Adding Geographic Coordinates\n",
    "The cleaned trade data is enriched with geographic information. A lookup file, `country-coord.csv`, is used to map latitude and longitude to both the importer and exporter of each trade record based on their ISO codes or country names. This step is crucial for later visualization and for calculating geographic distance if needed. Any trade records where coordinates could not be found for either the importer or the exporter are dropped from the dataset to ensure data integrity. The augmented DataFrame is then saved to a new temporary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add-coords-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_coords = pd.read_csv(TEMP_MERGED_CLEANED_FILE)\n",
    "df_country_coords_lookup = pd.read_csv(COORDS_FILE)\n",
    "\n",
    "alpha3_coords = df_country_coords_lookup.set_index('Alpha-3 code')[['Latitude (average)', 'Longitude (average)']].to_dict(orient='index')\n",
    "country_coords = df_country_coords_lookup.set_index('Country')[['Latitude (average)', 'Longitude (average)']].to_dict(orient='index')\n",
    "\n",
    "def get_coordinates(value):\n",
    "    if value in alpha3_coords:\n",
    "        return (alpha3_coords[value]['Latitude (average)'], \n",
    "                alpha3_coords[value]['Longitude (average)'])\n",
    "    elif value in country_coords:\n",
    "        return (country_coords[value]['Latitude (average)'], \n",
    "                country_coords[value]['Longitude (average)'])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "df_for_coords['importer_latitude'], df_for_coords['importer_longitude'] = zip(*df_for_coords['importer'].apply(get_coordinates))\n",
    "df_for_coords['exporter_latitude'], df_for_coords['exporter_longitude'] = zip(*df_for_coords['exporter'].apply(get_coordinates))\n",
    "\n",
    "df_for_coords.dropna(subset=['importer_latitude', 'importer_longitude', \n",
    "                               'exporter_latitude', 'exporter_longitude'], inplace=True)\n",
    "\n",
    "df_for_coords.to_csv(TEMP_WITH_COORDS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-n-md",
   "metadata": {},
   "source": [
    "## 5. \"Top N\" Partner Filtering and Final Save\n",
    "To focus the analysis on the most economically significant relationships and maintain computational feasibility, the dataset is filtered. The data is sorted by `year`, `importer`, and trade `amount` (descending). Then, for each importer within each year, only the top 20 trade partners (as defined by `TOP_N_PARTNERS`) are retained. This filtering step reduces the dataset to 17,170 core trade flows. The final, processed DataFrame for Part 1 is then saved to `trade_data.csv` to be used in subsequent analysis parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "top-n-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Top N DataFrame shape: (17170, 8)\n",
      "Final Top N DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17170 entries, 0 to 17169\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   importer            17170 non-null  object \n",
      " 1   exporter            17170 non-null  object \n",
      " 2   amount              17170 non-null  float64\n",
      " 3   year                17170 non-null  int64  \n",
      " 4   importer_latitude   17170 non-null  float64\n",
      " 5   importer_longitude  17170 non-null  float64\n",
      " 6   exporter_latitude   17170 non-null  float64\n",
      " 7   exporter_longitude  17170 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      "Final Top N DataFrame Head:\n",
      "    importer        exporter        amount  year  importer_latitude  \\\n",
      "0  Australia             USA  7.041979e+09  1988              -27.0   \n",
      "1  Australia           Japan  6.549632e+09  1988              -27.0   \n",
      "2  Australia  United Kingdom  2.436458e+09  1988              -27.0   \n",
      "3  Australia     New Zealand  1.423720e+09  1988              -27.0   \n",
      "4  Australia           Italy  1.008245e+09  1988              -27.0   \n",
      "\n",
      "   importer_longitude  exporter_latitude  exporter_longitude  \n",
      "0               133.0            38.0000            -97.0000  \n",
      "1               133.0            36.0000            138.0000  \n",
      "2               133.0            54.0000             -2.0000  \n",
      "3               133.0           -41.0000            174.0000  \n",
      "4               133.0            42.8333             12.8333  \n",
      "\n",
      "Final processed data for Part 1 saved to: trade_data.csv\n"
     ]
    }
   ],
   "source": [
    "df_for_topn_filter = pd.read_csv(TEMP_WITH_COORDS_FILE)\n",
    "df_for_topn_filter = df_for_topn_filter.sort_values(by=['year', 'importer', 'amount'], ascending=[True, True, False])\n",
    "df_final_part1 = df_for_topn_filter.groupby(['year', 'importer']).head(TOP_N_PARTNERS).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal Top N DataFrame shape: {df_final_part1.shape}\")\n",
    "print(\"Final Top N DataFrame Info:\")\n",
    "df_final_part1.info()\n",
    "print(\"\\nFinal Top N DataFrame Head:\")\n",
    "print(df_final_part1.head())\n",
    "\n",
    "df_final_part1.to_csv(FINAL_OUTPUT_FILE_PART1, index=False)\n",
    "print(f\"\\nFinal processed data for Part 1 saved to: {FINAL_OUTPUT_FILE_PART1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-md",
   "metadata": {},
   "source": [
    "## 6. Cleanup Temporary Files\n",
    "To maintain a clean project directory, the intermediate files (`temp_merged_cleaned_data.csv` and `temp_data_with_coords.csv`) that were created during the preprocessing pipeline are programmatically removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cleanup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully removed temporary file: temp_merged_cleaned_data.csv\n",
      "Successfully removed temporary file: temp_data_with_coords.csv\n",
      "\n",
      "Part 1 processing complete.\n"
     ]
    }
   ],
   "source": [
    "temp_files_to_remove = [TEMP_MERGED_CLEANED_FILE, TEMP_WITH_COORDS_FILE]\n",
    "for temp_file in temp_files_to_remove:\n",
    "    try:\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "            print(f\"Successfully removed temporary file: {temp_file}\")\n",
    "        else:\n",
    "            print(f\"Temporary file not found (already removed or never created): {temp_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing temporary file {temp_file}: {e}\")\n",
    "\n",
    "print(\"\\nPart 1 processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
