{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "part7-intro",
   "metadata": {},
   "source": [
    "# Part 7: Temporal Graph Network (TGN) Inspired Feature Engineering for Enhanced Forecasting\n",
    "\n",
    "This notebook explores the integration of features inspired by Temporal Graph Network (TGN) concepts. The objective is to generate dynamic node embeddings for countries and use them to enhance the performance of the best tree-based forecasting model (XGBoost) from Part 6.\n",
    "\n",
    "This experiment proved to be a significant success, demonstrating the value of learned graph representations. The methodology involved:\n",
    "1.  **Dynamic Graph Snapshot Preparation**: Loading the comprehensive dataset and transforming it into a sequence of **37 yearly graph snapshots** for **180 unique countries**.\n",
    "2.  **Dynamic Node Embedding Learning**: Implementing and training a simplified GCN-LSTM model for 100 epochs to learn time-aware embeddings for each country, generating **6,660 country-year embeddings**.\n",
    "3.  **Embedding Extraction and Lagging**: Extracting and lagging the learned node embeddings by one year to ensure they are suitable for forecasting.\n",
    "4.  **Forecasting Dataset Augmentation**: Merging these lagged TGN-inspired embeddings into the `X_train`, `X_val`, and `X_test` feature sets, increasing the feature count to **104**.\n",
    "5.  **Retraining and Evaluation of Best Model**: Retraining the best-performing XGBoost model from Part 6 using the newly augmented feature set.\n",
    "6.  **Comparative Performance Analysis**: Demonstrating the superior performance of the XGBoost model with TGN-inspired features (**RMSE: 3.74e+10**, **RÂ²: 0.3197**) against the original XGBoost model, establishing it as the new state-of-the-art model for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "part7-setup-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import LSTMCell, Linear, ReLU # Changed LSTM to LSTMCell for node-wise updates\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb # To retrain the best model from Part 6\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "FULL_DATA_FILE = 'trade_data_dynamic_features.csv' # From Part 4\n",
    "PROCESSED_DATA_DIR = 'processed_for_modeling/' # From Part 5\n",
    "MODELS_DIR = 'trained_models/' # From Part 6\n",
    "TGN_EMBEDDINGS_DIR = 'tgn_embeddings/'\n",
    "BEST_MODEL_FROM_PART6 = 'xgboost_model.joblib' # Assuming XGBoost was best and saved with this name\n",
    "MODEL_PERFORMANCE_FILE_PART6 = os.path.join(PROCESSED_DATA_DIR, 'model_performance_summary_final.csv') # From Part 6\n",
    "FINAL_MODEL_PERFORMANCE_FILE_PART7 = os.path.join(PROCESSED_DATA_DIR, 'model_performance_summary_part7.csv')\n",
    "\n",
    "TARGET_COLUMN_LOG = 'amount_log1p'\n",
    "\n",
    "# TGN specific parameters\n",
    "TGN_EMBEDDING_DIM = 32 \n",
    "TGN_HIDDEN_GCN_DIM = 64\n",
    "TGN_HIDDEN_LSTM_DIM = 64\n",
    "TGN_EPOCHS = 100\n",
    "TGN_LEARNING_RATE = 0.005\n",
    "MIN_YEAR_FOR_TGN = None \n",
    "MAX_YEAR_FOR_TGN = None \n",
    "TGN_TRAIN_END_YEAR_OFFSET = -3 \n",
    "TGN_VAL_END_YEAR_OFFSET = -1   \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "if not os.path.exists(TGN_EMBEDDINGS_DIR):\n",
    "    os.makedirs(TGN_EMBEDDINGS_DIR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-load-full-data",
   "metadata": {},
   "source": [
    "## 1. Load Full Data and Prepare for Graph Snapshots\n",
    "\n",
    "The initial step involves loading the comprehensive dataset from Part 4 (`trade_data_dynamic_features.csv`), which has a shape of (17170, 859). From this data, **180 unique countries** (nodes) are identified. Country names are then mapped to unique integer IDs, and the full range of years available (1988 to 2024) is determined for creating the sequence of graph snapshots needed for the TGN-inspired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "part7-load-data-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full data from trade_data_dynamic_features.csv. Shape: (17170, 859)\n",
      "Number of unique countries (nodes): 180\n",
      "Years for TGN processing: 1988 to 2024\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv(FULL_DATA_FILE)\n",
    "print(f\"Loaded full data from {FULL_DATA_FILE}. Shape: {df_full.shape}\")\n",
    "\n",
    "all_countries = sorted(list(set(df_full['importer']).union(set(df_full['exporter']))))\n",
    "country_to_id = {country: i for i, country in enumerate(all_countries)}\n",
    "id_to_country = {i: country for country, i in country_to_id.items()}\n",
    "num_nodes = len(all_countries)\n",
    "print(f\"Number of unique countries (nodes): {num_nodes}\")\n",
    "\n",
    "df_full['importer_id'] = df_full['importer'].map(country_to_id)\n",
    "df_full['exporter_id'] = df_full['exporter'].map(country_to_id)\n",
    "\n",
    "available_years = sorted(df_full['year'].unique())\n",
    "if MIN_YEAR_FOR_TGN is None: MIN_YEAR_FOR_TGN = min(available_years)\n",
    "if MAX_YEAR_FOR_TGN is None: MAX_YEAR_FOR_TGN = max(available_years)\n",
    "tgn_years = [yr for yr in available_years if MIN_YEAR_FOR_TGN <= yr <= MAX_YEAR_FOR_TGN]\n",
    "print(f\"Years for TGN processing: {min(tgn_years)} to {max(tgn_years)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-tgn-data-prep",
   "metadata": {},
   "source": [
    "## 2. TGN Data Preparation: Creating Yearly Graph Snapshots\n",
    "\n",
    "For each of the 37 years in the dataset, a graph snapshot is constructed. Each snapshot contains:\n",
    "-   **Node Features (`x`):** A matrix where each row represents a country. Features are derived from **9 lagged dynamic properties** (e.g., `importer_pagerank_dyn_lag1`, `importer_community_id_dyn_lag1`) from the main DataFrame, representing each country's state from the previous year. These features are scaled using `MinMaxScaler`.\n",
    "-   **Edge Index (`edge_index`):** A tensor representing the directed trade links (exporter to importer) active in the current year.\n",
    "-   **Edge Attributes (`edge_attr`):** The trade amount for each link in the current year, used as edge weights.\n",
    "-   **Auxiliary Target (`y`):** To train the GNN model, an auxiliary task is defined: predicting the log-transformed total export amount of each country in the *next* year. This encourages the model to learn embeddings that are predictive of future export performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "part7-snapshot-creation-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 features for TGN nodes (prefix: importer_): ['importer_pagerank_dyn_lag1', 'importer_hub_score_dyn_lag1', 'importer_authority_score_dyn_lag1', 'importer_harmonic_centrality_dyn_lag1', 'importer_betweenness_centrality_dyn_lag1', 'importer_pagerank_dyn_roll_mean_lag1', 'importer_pagerank_dyn_roll_std_lag1', 'importer_community_id_dyn_lag1', 'importer_community_stability_dyn_lag1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 37 yearly graph snapshots.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\4085813501.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n"
     ]
    }
   ],
   "source": [
    "node_feature_cols_candidates = [\n",
    "    'importer_pagerank_dyn_lag1', 'importer_hub_score_dyn_lag1', \n",
    "    'importer_authority_score_dyn_lag1', 'importer_harmonic_centrality_dyn_lag1',\n",
    "    'importer_betweenness_centrality_dyn_lag1', \n",
    "    'importer_pagerank_dyn_roll_mean_lag1', 'importer_pagerank_dyn_roll_std_lag1',\n",
    "    'importer_community_id_dyn_lag1', 'importer_community_stability_dyn_lag1'\n",
    "]\n",
    "node_feature_cols = [col for col in node_feature_cols_candidates if col in df_full.columns]\n",
    "if not node_feature_cols:\n",
    "    # Fallback if 'importer_' prefixed features are missing - try 'exporter_'\n",
    "    node_feature_cols_candidates_exp = [col.replace('importer_', 'exporter_') for col in node_feature_cols_candidates]\n",
    "    node_feature_cols = [col for col in node_feature_cols_candidates_exp if col in df_full.columns]\n",
    "    if not node_feature_cols:\n",
    "         raise ValueError(\"No suitable lagged node feature columns (neither importer_ nor exporter_ prefixed) found for TGN. Check Part 4 outputs.\")\n",
    "    feature_prefix_tgn = 'exporter_'\n",
    "else:\n",
    "    feature_prefix_tgn = 'importer_'\n",
    "print(f\"Using {len(node_feature_cols)} features for TGN nodes (prefix: {feature_prefix_tgn}): {node_feature_cols}\")\n",
    "\n",
    "yearly_snapshots = []\n",
    "for year_t in tgn_years:\n",
    "    df_year_t = df_full[df_full['year'] == year_t]\n",
    "    if df_year_t.empty: continue\n",
    "\n",
    "    x_t_df = pd.DataFrame(index=range(num_nodes), columns=node_feature_cols).fillna(0.0)\n",
    "    # Create country features by taking mean of selected features if a country appears multiple times (e.g. as importer for diff. exporters)\n",
    "    # This uses the 'importer_id' (or 'exporter_id' based on prefix) for grouping to get country-specific features for the year\n",
    "    country_level_node_features = df_year_t.groupby(feature_prefix_tgn.replace('_','') + '_id')[node_feature_cols].mean()\n",
    "    for node_id, features in country_level_node_features.iterrows():\n",
    "        if int(node_id) < num_nodes: # Check node_id is valid, can happen if mapping has issues\n",
    "            x_t_df.loc[int(node_id)] = features.values\n",
    "    x_t = x_t_df.values\n",
    "\n",
    "    edge_data_t = df_year_t[['exporter_id', 'importer_id', 'amount_lag_1']].dropna()\n",
    "    # Filter out edges where exporter_id or importer_id might be NaN after mapping if some countries were not in 'all_countries'\n",
    "    edge_data_t = edge_data_t[edge_data_t['exporter_id'].notna() & edge_data_t['importer_id'].notna()]\n",
    "    source_nodes = edge_data_t['exporter_id'].values.astype(int)\n",
    "    target_nodes = edge_data_t['importer_id'].values.astype(int)\n",
    "    edge_index_t = torch.tensor(np.array([source_nodes, target_nodes]), dtype=torch.long)\n",
    "    # Use 'amount' from t as edge weight, not 'amount_lag_1' for GCN processing if TGN aims to predict t+1 from state t\n",
    "    edge_attr_t_values = df_year_t.loc[edge_data_t.index, 'amount'].fillna(0).values # Use current year's amount for edge weight\n",
    "    edge_attr_t = torch.tensor(edge_attr_t_values, dtype=torch.float).unsqueeze(1) \n",
    "\n",
    "    y_t_aux = np.zeros(num_nodes)\n",
    "    if (year_t + 1) in df_full['year'].unique():\n",
    "        df_year_t_plus_1 = df_full[df_full['year'] == (year_t + 1)]\n",
    "        total_exports_t_plus_1 = df_year_t_plus_1.groupby('exporter_id')['amount'].sum().reindex(range(num_nodes), fill_value=0.0)\n",
    "        y_t_aux = np.log1p(total_exports_t_plus_1.values)\n",
    "    \n",
    "    scaler_x = MinMaxScaler()\n",
    "    x_t_scaled = scaler_x.fit_transform(x_t)\n",
    "\n",
    "    snapshot = Data(x=torch.tensor(x_t_scaled, dtype=torch.float),\n",
    "                      edge_index=edge_index_t,\n",
    "                      edge_attr=edge_attr_t,\n",
    "                      y=torch.tensor(y_t_aux, dtype=torch.float).unsqueeze(1))\n",
    "    yearly_snapshots.append(snapshot)\n",
    "\n",
    "print(f\"Created {len(yearly_snapshots)} yearly graph snapshots.\")\n",
    "if not yearly_snapshots: raise ValueError(\"No snapshots created, TGN part cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-tgn-model-def",
   "metadata": {},
   "source": [
    "## 3. Dynamic Graph Embedding Model (GCN-LSTM)\n",
    "\n",
    "A GCN-LSTM model is implemented to learn dynamic node embeddings. This architecture combines a Graph Convolutional Network (GCN) to capture structural information from each yearly snapshot with an LSTMCell to model temporal dependencies across snapshots.\n",
    "\n",
    "-   **GCN Layer**: Processes a snapshot's node features and graph structure to produce intermediate node representations.\n",
    "-   **LSTMCell**: Takes the GCN output and the previous hidden/cell state as input to update each node's state, capturing its temporal evolution.\n",
    "-   **Embedding & Prediction Layers**: Fully connected layers transform the LSTM hidden state into the final 32-dimensional embedding and predict the auxiliary target (next year's total exports).\n",
    "\n",
    "The model is trained for 100 epochs on the training snapshots (1988-2021). The primary goal is not the auxiliary prediction, but the generation of meaningful `node_embeddings` for each country at each time step, resulting in **6,660** node-year embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "part7-gcn-lstm-model-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGN: 34 train snapshots, 2 validation snapshots.\n",
      "\n",
      "Training TGN model to learn dynamic node embeddings...\n",
      "TGN Epoch 1/100, Train Loss: 98.6575\n",
      "TGN Epoch 2/100, Train Loss: 96.7193\n",
      "TGN Epoch 3/100, Train Loss: 99.0228\n",
      "TGN Epoch 4/100, Train Loss: 93.0902\n",
      "TGN Epoch 5/100, Train Loss: 87.8955\n",
      "TGN Epoch 6/100, Train Loss: 86.2498\n",
      "TGN Epoch 7/100, Train Loss: 86.8072\n",
      "TGN Epoch 8/100, Train Loss: 89.4779\n",
      "TGN Epoch 9/100, Train Loss: 86.7113\n",
      "TGN Epoch 10/100, Train Loss: 84.4850\n",
      "TGN Epoch 11/100, Train Loss: 83.8829\n",
      "TGN Epoch 12/100, Train Loss: 83.6894\n",
      "TGN Epoch 13/100, Train Loss: 83.1337\n",
      "TGN Epoch 14/100, Train Loss: 82.6392\n",
      "TGN Epoch 15/100, Train Loss: 82.1594\n",
      "TGN Epoch 16/100, Train Loss: 81.9208\n",
      "TGN Epoch 17/100, Train Loss: 81.7096\n",
      "TGN Epoch 18/100, Train Loss: 81.7639\n",
      "TGN Epoch 19/100, Train Loss: 81.6598\n",
      "TGN Epoch 20/100, Train Loss: 81.2763\n",
      "TGN Epoch 21/100, Train Loss: 80.4632\n",
      "TGN Epoch 22/100, Train Loss: 79.8077\n",
      "TGN Epoch 23/100, Train Loss: 79.7858\n",
      "TGN Epoch 24/100, Train Loss: 79.1958\n",
      "TGN Epoch 25/100, Train Loss: 79.4701\n",
      "TGN Epoch 26/100, Train Loss: 79.5195\n",
      "TGN Epoch 27/100, Train Loss: 79.2762\n",
      "TGN Epoch 28/100, Train Loss: 79.3639\n",
      "TGN Epoch 29/100, Train Loss: 79.6392\n",
      "TGN Epoch 30/100, Train Loss: 81.3195\n",
      "TGN Epoch 31/100, Train Loss: 78.9944\n",
      "TGN Epoch 32/100, Train Loss: 78.7924\n",
      "TGN Epoch 33/100, Train Loss: 78.8179\n",
      "TGN Epoch 34/100, Train Loss: 78.4610\n",
      "TGN Epoch 35/100, Train Loss: 78.0799\n",
      "TGN Epoch 36/100, Train Loss: 78.7196\n",
      "TGN Epoch 37/100, Train Loss: 78.4825\n",
      "TGN Epoch 38/100, Train Loss: 78.3615\n",
      "TGN Epoch 39/100, Train Loss: 78.5844\n",
      "TGN Epoch 40/100, Train Loss: 77.8313\n",
      "TGN Epoch 41/100, Train Loss: 77.5279\n",
      "TGN Epoch 42/100, Train Loss: 77.1112\n",
      "TGN Epoch 43/100, Train Loss: 77.5548\n",
      "TGN Epoch 44/100, Train Loss: 77.2729\n",
      "TGN Epoch 45/100, Train Loss: 76.9092\n",
      "TGN Epoch 46/100, Train Loss: 77.0017\n",
      "TGN Epoch 47/100, Train Loss: 77.7401\n",
      "TGN Epoch 48/100, Train Loss: 77.0882\n",
      "TGN Epoch 49/100, Train Loss: 77.2039\n",
      "TGN Epoch 50/100, Train Loss: 76.7015\n",
      "TGN Epoch 51/100, Train Loss: 76.5321\n",
      "TGN Epoch 52/100, Train Loss: 76.9825\n",
      "TGN Epoch 53/100, Train Loss: 77.4659\n",
      "TGN Epoch 54/100, Train Loss: 77.1513\n",
      "TGN Epoch 55/100, Train Loss: 76.6183\n",
      "TGN Epoch 56/100, Train Loss: 77.0035\n",
      "TGN Epoch 57/100, Train Loss: 77.0339\n",
      "TGN Epoch 58/100, Train Loss: 76.4041\n",
      "TGN Epoch 59/100, Train Loss: 75.9926\n",
      "TGN Epoch 60/100, Train Loss: 75.6999\n",
      "TGN Epoch 61/100, Train Loss: 75.7189\n",
      "TGN Epoch 62/100, Train Loss: 75.6917\n",
      "TGN Epoch 63/100, Train Loss: 75.7096\n",
      "TGN Epoch 64/100, Train Loss: 75.4113\n",
      "TGN Epoch 65/100, Train Loss: 75.9915\n",
      "TGN Epoch 66/100, Train Loss: 76.1355\n",
      "TGN Epoch 67/100, Train Loss: 75.9846\n",
      "TGN Epoch 68/100, Train Loss: 75.7263\n",
      "TGN Epoch 69/100, Train Loss: 75.6602\n",
      "TGN Epoch 70/100, Train Loss: 75.9244\n",
      "TGN Epoch 71/100, Train Loss: 76.4067\n",
      "TGN Epoch 72/100, Train Loss: 75.7541\n",
      "TGN Epoch 73/100, Train Loss: 75.1841\n",
      "TGN Epoch 74/100, Train Loss: 75.3269\n",
      "TGN Epoch 75/100, Train Loss: 74.8817\n",
      "TGN Epoch 76/100, Train Loss: 75.1715\n",
      "TGN Epoch 77/100, Train Loss: 75.0069\n",
      "TGN Epoch 78/100, Train Loss: 75.0612\n",
      "TGN Epoch 79/100, Train Loss: 75.0214\n",
      "TGN Epoch 80/100, Train Loss: 74.9023\n",
      "TGN Epoch 81/100, Train Loss: 75.3564\n",
      "TGN Epoch 82/100, Train Loss: 75.3003\n",
      "TGN Epoch 83/100, Train Loss: 75.6754\n",
      "TGN Epoch 84/100, Train Loss: 75.2279\n",
      "TGN Epoch 85/100, Train Loss: 75.1957\n",
      "TGN Epoch 86/100, Train Loss: 75.1632\n",
      "TGN Epoch 87/100, Train Loss: 74.7560\n",
      "TGN Epoch 88/100, Train Loss: 74.9712\n",
      "TGN Epoch 89/100, Train Loss: 74.8737\n",
      "TGN Epoch 90/100, Train Loss: 75.1996\n",
      "TGN Epoch 91/100, Train Loss: 75.2163\n",
      "TGN Epoch 92/100, Train Loss: 75.2138\n",
      "TGN Epoch 93/100, Train Loss: 74.6002\n",
      "TGN Epoch 94/100, Train Loss: 74.6459\n",
      "TGN Epoch 95/100, Train Loss: 74.9975\n",
      "TGN Epoch 96/100, Train Loss: 74.8067\n",
      "TGN Epoch 97/100, Train Loss: 75.2442\n",
      "TGN Epoch 98/100, Train Loss: 75.0628\n",
      "TGN Epoch 99/100, Train Loss: 76.0090\n",
      "TGN Epoch 100/100, Train Loss: 75.8753\n",
      "\n",
      "Generating dynamic node embeddings for all years...\n",
      "Generated 6660 node-year embeddings.\n"
     ]
    }
   ],
   "source": [
    "class GCNLSTM(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, gcn_hidden_dim, lstm_hidden_dim, embedding_dim):\n",
    "        super(GCNLSTM, self).__init__()\n",
    "        self.gcn = GCNConv(num_node_features, gcn_hidden_dim)\n",
    "        self.lstm_cell = LSTMCell(gcn_hidden_dim, lstm_hidden_dim)\n",
    "        self.fc_embed = Linear(lstm_hidden_dim, embedding_dim)\n",
    "        self.fc_predict = Linear(embedding_dim, 1) \n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, snapshot_data, h_prev, c_prev):\n",
    "        x, edge_index, edge_attr = snapshot_data.x, snapshot_data.edge_index, snapshot_data.edge_attr\n",
    "        edge_weight = edge_attr.squeeze() if edge_attr is not None and edge_attr.numel() > 0 else None\n",
    "        gcn_out = self.relu(self.gcn(x, edge_index, edge_weight=edge_weight))\n",
    "        h_curr, c_curr = self.lstm_cell(gcn_out, (h_prev, c_prev))\n",
    "        node_embeddings_t = self.relu(self.fc_embed(h_curr))\n",
    "        predictions_t = self.fc_predict(node_embeddings_t)\n",
    "        return predictions_t, node_embeddings_t, h_curr, c_curr\n",
    "    \n",
    "    def init_hidden_state(self, num_nodes_in_snapshot):\n",
    "        return (torch.zeros(num_nodes_in_snapshot, self.lstm_hidden_dim).to(device),\n",
    "                torch.zeros(num_nodes_in_snapshot, self.lstm_hidden_dim).to(device))\n",
    "\n",
    "tgn_train_split_idx = int(len(tgn_years) + TGN_TRAIN_END_YEAR_OFFSET) \n",
    "tgn_val_split_idx = int(len(tgn_years) + TGN_VAL_END_YEAR_OFFSET)    \n",
    "\n",
    "train_snapshots = yearly_snapshots[:tgn_train_split_idx]\n",
    "val_snapshots = yearly_snapshots[tgn_train_split_idx:tgn_val_split_idx]\n",
    "print(f\"TGN: {len(train_snapshots)} train snapshots, {len(val_snapshots)} validation snapshots.\")\n",
    "\n",
    "df_tgn_embeddings = pd.DataFrame() # Initialize to prevent error if training is skipped\n",
    "\n",
    "if not train_snapshots:\n",
    "    print(\"Warning: No training snapshots for TGN. Skipping TGN model training and embedding generation.\")\n",
    "else:\n",
    "    num_features_tgn = train_snapshots[0].x.shape[1]\n",
    "    tgn_model = GCNLSTM(num_node_features=num_features_tgn, \n",
    "                        gcn_hidden_dim=TGN_HIDDEN_GCN_DIM, \n",
    "                        lstm_hidden_dim=TGN_HIDDEN_LSTM_DIM, \n",
    "                        embedding_dim=TGN_EMBEDDING_DIM).to(device)\n",
    "    optimizer_tgn = torch.optim.Adam(tgn_model.parameters(), lr=TGN_LEARNING_RATE)\n",
    "    criterion_tgn = torch.nn.MSELoss()\n",
    "\n",
    "    print(\"\\nTraining TGN model to learn dynamic node embeddings...\")\n",
    "    for epoch in range(TGN_EPOCHS):\n",
    "        tgn_model.train()\n",
    "        epoch_loss = 0\n",
    "        h, c = tgn_model.init_hidden_state(num_nodes) # num_nodes is the total unique countries\n",
    "        for snapshot in train_snapshots:\n",
    "            snapshot = snapshot.to(device)\n",
    "            # Ensure h, c are correctly sized for the current snapshot's number of nodes (which is fixed at num_nodes)\n",
    "            if h.shape[0] != snapshot.x.shape[0]: \n",
    "                 h, c = tgn_model.init_hidden_state(snapshot.x.shape[0])\n",
    "            \n",
    "            optimizer_tgn.zero_grad()\n",
    "            predictions, _, h_new, c_new = tgn_model(snapshot, h.detach(), c.detach())\n",
    "            h, c = h_new, c_new\n",
    "            loss = criterion_tgn(predictions, snapshot.y)\n",
    "            loss.backward()\n",
    "            optimizer_tgn.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_epoch_loss = epoch_loss / len(train_snapshots) if len(train_snapshots) > 0 else 0\n",
    "        print(f\"TGN Epoch {epoch+1}/{TGN_EPOCHS}, Train Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nGenerating dynamic node embeddings for all years...\")\n",
    "    tgn_model.eval()\n",
    "    dynamic_node_embeddings_over_time_list = [] \n",
    "    with torch.no_grad():\n",
    "        h, c = tgn_model.init_hidden_state(num_nodes)\n",
    "        for i, snapshot in enumerate(yearly_snapshots):\n",
    "            current_data_year = tgn_years[i]\n",
    "            snapshot = snapshot.to(device)\n",
    "            if h.shape[0] != snapshot.x.shape[0]:\n",
    "                 h, c = tgn_model.init_hidden_state(snapshot.x.shape[0])\n",
    "\n",
    "            _, node_embeds, h_new, c_new = tgn_model(snapshot, h, c)\n",
    "            h, c = h_new, c_new\n",
    "            for node_idx in range(snapshot.x.shape[0]): # Iterate up to actual nodes in snapshot (num_nodes)\n",
    "                dynamic_node_embeddings_over_time_list.append({\n",
    "                    'year': current_data_year, \n",
    "                    'node_id': node_idx,\n",
    "                    'country': id_to_country.get(node_idx, f'Unknown_Node_{node_idx}'),\n",
    "                    **{f'tgn_emb_{j}': embed_val for j, embed_val in enumerate(node_embeds[node_idx].cpu().numpy())}\n",
    "                })\n",
    "    df_tgn_embeddings = pd.DataFrame(dynamic_node_embeddings_over_time_list)\n",
    "    if not df_tgn_embeddings.empty:\n",
    "        print(f\"Generated {df_tgn_embeddings.shape[0]} node-year embeddings.\")\n",
    "        df_tgn_embeddings.to_csv(os.path.join(TGN_EMBEDDINGS_DIR, 'tgn_node_embeddings_yearly.csv'), index=False)\n",
    "    else:\n",
    "        print(\"No TGN embeddings were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-lag-augment",
   "metadata": {},
   "source": [
    "## 4. Lag Embeddings and Augment Forecasting Dataset\n",
    "\n",
    "The generated TGN node embeddings represent the state of each country at year `t`. To use them for forecasting, these embeddings are lagged by one year to ensure only past information is used. This process involves:\n",
    "1.  Creating `_lag1` versions of all 32 embedding dimensions for each country.\n",
    "2.  Merging the lagged TGN embeddings into the feature sets twice: once for the importer and once for the exporter.\n",
    "3.  Filling any resulting `NaN` values with 0. The final augmented feature sets (`X_train_aug`, etc.) now contain **104** columns.\n",
    "4.  **Saving the augmented data splits** (`X_train_aug.csv`, etc.) for use in the final interpretation notebook (Part 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "part7-lag-augment-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing 'importer' and 'exporter' for y_test_full_info from 'trade_pair_id'.\n",
      "\n",
      "Lagging TGN embeddings by 1 year...\n",
      "\n",
      "Augmenting X_train, X_val, X_test with TGN embeddings...\n",
      "X_train_aug shape: (14000, 104)\n",
      "X_val_aug shape: (2350, 104)\n",
      "X_test_aug shape: (820, 104)\n",
      "\n",
      "Saving augmented data splits for Part 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
      "C:\\Users\\Askeladd\\AppData\\Local\\Temp\\ipykernel_16964\\3287926675.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_augmented[new_emb_col_name].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data splits saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # Assuming this is used elsewhere if not directly here for scaler\n",
    "\n",
    "# Constants matching Part 5 configuration (adjust if they were different in your Part 5)\n",
    "TRAIN_END_YEAR_PART5 = 2020\n",
    "VALIDATION_END_YEAR_PART5 = 2022\n",
    "\n",
    "# Load X dataframes\n",
    "X_train_final_loaded = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'X_train.csv'))\n",
    "X_val_final_loaded = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'X_val.csv'))\n",
    "X_test_final_loaded = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'X_test.csv'))\n",
    "\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    raise ValueError(\"The 'df_full' DataFrame (from trade_data_dynamic_features.csv) is not loaded or is empty. Cannot proceed.\")\n",
    "\n",
    "required_id_cols_df_full = ['year', 'importer', 'exporter', 'trade_pair_id']\n",
    "if not all(col in df_full.columns for col in required_id_cols_df_full):\n",
    "    raise ValueError(f\"df_full is missing one or more required columns for ID reconstruction: {required_id_cols_df_full}. Available: {df_full.columns.tolist()}\")\n",
    "\n",
    "df_full_train_subset_info = df_full[df_full['year'] <= TRAIN_END_YEAR_PART5][required_id_cols_df_full].copy()\n",
    "df_full_val_subset_info = df_full[(df_full['year'] > TRAIN_END_YEAR_PART5) & (df_full['year'] <= VALIDATION_END_YEAR_PART5)][required_id_cols_df_full].copy()\n",
    "\n",
    "if len(df_full_train_subset_info) >= len(X_train_final_loaded):\n",
    "    y_train_full_info_constructed = df_full_train_subset_info.iloc[:len(X_train_final_loaded)].reset_index(drop=True)\n",
    "else:\n",
    "    raise ValueError(f\"Mismatch in row count for training data. df_full_train_subset_info has {len(df_full_train_subset_info)} rows, \"\n",
    "                     f\"but X_train_final_loaded has {len(X_train_final_loaded)} rows. Check year splits and data consistency.\")\n",
    "\n",
    "if len(df_full_val_subset_info) >= len(X_val_final_loaded):\n",
    "    y_val_full_info_constructed = df_full_val_subset_info.iloc[:len(X_val_final_loaded)].reset_index(drop=True)\n",
    "else:\n",
    "    raise ValueError(f\"Mismatch in row count for validation data. df_full_val_subset_info has {len(df_full_val_subset_info)} rows, \"\n",
    "                     f\"but X_val_final_loaded has {len(X_val_final_loaded)} rows. Check year splits and data consistency.\")\n",
    "\n",
    "# --- Load and Prepare y_test_full_info ---\n",
    "try:\n",
    "    y_test_full_info_loaded = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'y_test_full_info.csv'))\n",
    "    if 'importer' not in y_test_full_info_loaded.columns or 'exporter' not in y_test_full_info_loaded.columns:\n",
    "        if 'trade_pair_id' in y_test_full_info_loaded.columns:\n",
    "            print(\"Reconstructing 'importer' and 'exporter' for y_test_full_info from 'trade_pair_id'.\")\n",
    "            y_test_full_info_loaded[['importer', 'exporter']] = y_test_full_info_loaded['trade_pair_id'].str.split('_', expand=True)\n",
    "        else:\n",
    "            raise ValueError(\"'y_test_full_info.csv' is missing 'importer'/'exporter' and also 'trade_pair_id' for reconstruction.\")\n",
    "    y_test_full_info_prepared = y_test_full_info_loaded\n",
    "except FileNotFoundError as e_test:\n",
    "    print(f\"ERROR: 'y_test_full_info.csv' not found in {PROCESSED_DATA_DIR}. This file is essential for test set augmentation.\")\n",
    "    raise e_test\n",
    "\n",
    "\n",
    "if 'df_tgn_embeddings' not in locals() or df_tgn_embeddings.empty:\n",
    "    print(\"TGN embeddings DataFrame not found or empty. Skipping augmentation.\")\n",
    "    X_train_aug, X_val_aug, X_test_aug = X_train_final_loaded.copy(), X_val_final_loaded.copy(), X_test_final_loaded.copy()\n",
    "else:\n",
    "    print(\"\\nLagging TGN embeddings by 1 year...\")\n",
    "    tgn_embedding_cols = [col for col in df_tgn_embeddings.columns if col.startswith('tgn_emb_')]\n",
    "    \n",
    "    if not all(c in df_tgn_embeddings.columns for c in ['country', 'year']):\n",
    "        raise ValueError(\"'df_tgn_embeddings' must contain 'country' and 'year' columns for sorting and merging.\")\n",
    "\n",
    "    df_tgn_embeddings_lagged = df_tgn_embeddings.sort_values(['country', 'year']).copy()\n",
    "    for col in tgn_embedding_cols:\n",
    "        df_tgn_embeddings_lagged[f'{col}_lag1'] = df_tgn_embeddings_lagged.groupby('country')[col].shift(1)\n",
    "    \n",
    "    lagged_emb_cols_to_merge = ['country', 'year'] + [f'{col}_lag1' for col in tgn_embedding_cols]\n",
    "    df_tgn_embeddings_for_merge = df_tgn_embeddings_lagged[lagged_emb_cols_to_merge].dropna()\n",
    "\n",
    "    def augment_with_tgn_embeddings(X_df, y_full_info_df_to_use, tgn_embeddings_to_merge_df, role_prefix):\n",
    "        X_df_with_ids = X_df.copy()\n",
    "        \n",
    "        required_y_info_cols = ['year', role_prefix.replace('_','')]\n",
    "        if not all(col in y_full_info_df_to_use.columns for col in required_y_info_cols):\n",
    "            raise ValueError(f\"The provided y_full_info_df (for role {role_prefix}) is missing one of required columns: {required_y_info_cols}. \"\n",
    "                             f\"Available: {y_full_info_df_to_use.columns.tolist()}\")\n",
    "\n",
    "        X_df_with_ids['year'] = y_full_info_df_to_use['year'].values \n",
    "        X_df_with_ids['country_to_merge'] = y_full_info_df_to_use[role_prefix.replace('_','')].values\n",
    "        \n",
    "        emb_cols_to_rename_and_merge = {}\n",
    "        for col in tgn_embeddings_to_merge_df.columns:\n",
    "            if col.startswith('tgn_emb_') and col.endswith('_lag1'):\n",
    "                emb_cols_to_rename_and_merge[col] = f'{role_prefix}{col}'\n",
    "        \n",
    "        tgn_embeddings_copy_for_rename = tgn_embeddings_to_merge_df.copy()\n",
    "        tgn_embeddings_renamed = tgn_embeddings_copy_for_rename.rename(columns=emb_cols_to_rename_and_merge)\n",
    "        \n",
    "        actual_merge_cols_from_embeddings = ['country', 'year'] + list(emb_cols_to_rename_and_merge.values())\n",
    "        \n",
    "        X_augmented = pd.merge(X_df_with_ids, \n",
    "                               tgn_embeddings_renamed[actual_merge_cols_from_embeddings], \n",
    "                               left_on=['country_to_merge', 'year'], \n",
    "                               right_on=['country', 'year'], \n",
    "                               how='left')\n",
    "        \n",
    "        for new_emb_col_name in emb_cols_to_rename_and_merge.values():\n",
    "            if new_emb_col_name in X_augmented.columns:\n",
    "                X_augmented[new_emb_col_name].fillna(0, inplace=True)\n",
    "            else: \n",
    "                X_augmented[new_emb_col_name] = 0 \n",
    "        \n",
    "        X_augmented.drop(columns=['country_to_merge', 'country', 'year_y' if 'year_y' in X_augmented.columns else None, 'year'], errors='ignore', inplace=True)\n",
    "\n",
    "        final_columns_present = X_df.columns.tolist() + [col for col in emb_cols_to_rename_and_merge.values() if col in X_augmented.columns]\n",
    "        \n",
    "        seen = set()\n",
    "        unique_final_cols = [x for x in final_columns_present if not (x in seen or seen.add(x))]\n",
    "\n",
    "        return X_augmented[unique_final_cols]\n",
    "\n",
    "    print(\"\\nAugmenting X_train, X_val, X_test with TGN embeddings...\")\n",
    "    X_train_aug_imp = augment_with_tgn_embeddings(X_train_final_loaded, y_train_full_info_constructed, df_tgn_embeddings_for_merge, 'importer_')\n",
    "    X_train_aug = augment_with_tgn_embeddings(X_train_aug_imp, y_train_full_info_constructed, df_tgn_embeddings_for_merge, 'exporter_')\n",
    "\n",
    "    X_val_aug_imp = augment_with_tgn_embeddings(X_val_final_loaded, y_val_full_info_constructed, df_tgn_embeddings_for_merge, 'importer_')\n",
    "    X_val_aug = augment_with_tgn_embeddings(X_val_aug_imp, y_val_full_info_constructed, df_tgn_embeddings_for_merge, 'exporter_')\n",
    "\n",
    "    X_test_aug_imp = augment_with_tgn_embeddings(X_test_final_loaded, y_test_full_info_prepared, df_tgn_embeddings_for_merge, 'importer_') \n",
    "    X_test_aug = augment_with_tgn_embeddings(X_test_aug_imp, y_test_full_info_prepared, df_tgn_embeddings_for_merge, 'exporter_')\n",
    "\n",
    "    print(f\"X_train_aug shape: {X_train_aug.shape}\")\n",
    "    print(f\"X_val_aug shape: {X_val_aug.shape}\")\n",
    "    print(f\"X_test_aug shape: {X_test_aug.shape}\")\n",
    "    \n",
    "    # --- SAVE THE AUGMENTED DATA SPLITS FOR PART 8 ---\n",
    "    print(\"\\nSaving augmented data splits for Part 8...\")\n",
    "    X_train_aug.to_csv(os.path.join(PROCESSED_DATA_DIR, 'X_train_aug.csv'), index=False)\n",
    "    X_val_aug.to_csv(os.path.join(PROCESSED_DATA_DIR, 'X_val_aug.csv'), index=False)\n",
    "    X_test_aug.to_csv(os.path.join(PROCESSED_DATA_DIR, 'X_test_aug.csv'), index=False)\n",
    "    print(\"Augmented data splits saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-retrain-xgb",
   "metadata": {},
   "source": [
    "## 5. Retrain Best Model (XGBoost) with Augmented Features\n",
    "\n",
    "The best-performing model from Part 6, XGBoost, is retrained using the augmented feature sets (`X_train_aug`, `X_val_aug`, `X_test_aug`). To isolate the impact of the TGN-inspired features, the model's hyperparameters are kept consistent with those determined previously. The retrained model is then evaluated on the test set, demonstrating a notable improvement and achieving an **RMSE of 3.74e+10** and an **R-squared of 0.3197**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "part7-retrain-xgb-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model performance summary from Part 6 (6 models).\n",
      "\n",
      "--- Retraining XGBoost Model with TGN Augmented Features ---\n",
      "Using best parameters from saved XGBoost model (xgboost_model.joblib) from Part 6.\n",
      "--- XGBoost + TGN Emb (Test) Evaluation ---\n",
      "RMSE (Original Scale): 37376045181.78\n",
      "MAE (Original Scale):  6197175325.39\n",
      "R-squared (Original Scale): 0.3197\n",
      "XGBoost model with TGN augmented features saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_predictions_part7(y_true_log, y_pred_log, model_name):\n",
    "    y_true_original = np.expm1(y_true_log)\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "    y_pred_original = np.maximum(0, y_pred_original)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n",
    "    mae = mean_absolute_error(y_true_original, y_pred_original)\n",
    "    r2 = r2_score(y_true_original, y_pred_original)\n",
    "    print(f\"--- {model_name} Evaluation ---\")\n",
    "    print(f\"RMSE (Original Scale): {rmse:.2f}\")\n",
    "    print(f\"MAE (Original Scale):  {mae:.2f}\")\n",
    "    print(f\"R-squared (Original Scale): {r2:.4f}\")\n",
    "    return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "y_train_log = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'y_train_log.csv'))[TARGET_COLUMN_LOG]\n",
    "y_val_log = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'y_val_log.csv'))[TARGET_COLUMN_LOG]\n",
    "y_test_log = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'y_test_log.csv'))[TARGET_COLUMN_LOG]\n",
    "\n",
    "\n",
    "model_performance_summary_part7 = []\n",
    "if os.path.exists(MODEL_PERFORMANCE_FILE_PART6):\n",
    "    df_perf_part6 = pd.read_csv(MODEL_PERFORMANCE_FILE_PART6)\n",
    "    model_performance_summary_part7 = df_perf_part6.to_dict('records')\n",
    "    print(f\"Loaded model performance summary from Part 6 ({len(model_performance_summary_part7)} models).\")\n",
    "else:\n",
    "    print(f\"Warning: {MODEL_PERFORMANCE_FILE_PART6} not found. Starting new performance summary.\")\n",
    "\n",
    "best_xgb_augmented = None\n",
    "\n",
    "if 'X_train_aug' in locals() and X_train_aug is not None and not X_train_aug.empty and \\\n",
    "   'X_val_aug' in locals() and X_val_aug is not None and not X_val_aug.empty:\n",
    "    print(\"\\n--- Retraining XGBoost Model with TGN Augmented Features ---\")\n",
    "    try:\n",
    "        best_xgb_part6 = joblib.load(os.path.join(MODELS_DIR, BEST_MODEL_FROM_PART6))\n",
    "        best_xgb_params_part6 = best_xgb_part6.get_params()\n",
    "        print(f\"Using best parameters from saved XGBoost model ({BEST_MODEL_FROM_PART6}) from Part 6.\")\n",
    "\n",
    "        if 'objective' not in best_xgb_params_part6 or best_xgb_params_part6['objective'] is None:\n",
    "            best_xgb_params_part6['objective'] = 'reg:squarederror'\n",
    "        if 'random_state' not in best_xgb_params_part6 or best_xgb_params_part6['random_state'] is None:\n",
    "             best_xgb_params_part6['random_state'] = RANDOM_SEED\n",
    "             \n",
    "        params_to_remove = ['callbacks', 'early_stopping_rounds'] \n",
    "        for param_key in params_to_remove:\n",
    "            if param_key in best_xgb_params_part6:\n",
    "                del best_xgb_params_part6[param_key]\n",
    "\n",
    "        xgb_aug = xgb.XGBRegressor(**best_xgb_params_part6)\n",
    "        xgb_aug.fit(X_train_aug, y_train_log,\n",
    "                    eval_set=[(X_val_aug, y_val_log)],\n",
    "                    verbose=False)\n",
    "        best_xgb_augmented = xgb_aug\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Saved XGBoost model ({BEST_MODEL_FROM_PART6}) from Part 6 not found. Using default XGBoost parameters for augmented data.\")\n",
    "        best_xgb_augmented = xgb.XGBRegressor(random_state=RANDOM_SEED, n_estimators=500, learning_rate=0.05, max_depth=5,\n",
    "                                              objective='reg:squarederror', n_jobs=-1)\n",
    "        best_xgb_augmented.fit(X_train_aug, y_train_log,\n",
    "                               eval_set=[(X_val_aug, y_val_log)],\n",
    "                               verbose=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during XGBoost retraining: {e}\")\n",
    "        best_xgb_augmented = None\n",
    "\n",
    "\n",
    "    if 'X_test_aug' in locals() and X_test_aug is not None and not X_test_aug.empty and best_xgb_augmented is not None:\n",
    "        y_pred_log_xgb_aug_test = best_xgb_augmented.predict(X_test_aug)\n",
    "        xgb_aug_metrics = evaluate_predictions_part7(y_test_log, y_pred_log_xgb_aug_test, \"XGBoost + TGN Emb (Test)\")\n",
    "        \n",
    "        model_performance_summary_part7 = [m for m in model_performance_summary_part7 if m['Model'] != \"XGBoost + TGN Emb (Test)\"]\n",
    "        model_performance_summary_part7.append(xgb_aug_metrics)\n",
    "        \n",
    "        joblib.dump(best_xgb_augmented, os.path.join(MODELS_DIR, 'xgboost_tgn_augmented_model.joblib'))\n",
    "        print(\"XGBoost model with TGN augmented features saved.\")\n",
    "    elif best_xgb_augmented is None:\n",
    "        print(\"Skipping test set prediction because augmented XGBoost model training failed.\")\n",
    "    else:\n",
    "        print(\"Skipping test set prediction because X_test_aug is empty or not defined.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping XGBoost retraining with augmented features as X_train_aug or X_val_aug is empty or not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-final-compare",
   "metadata": {},
   "source": [
    "## 6. Final Model Comparison and Conclusion\n",
    "\n",
    "The performance of the XGBoost model augmented with TGN-inspired embeddings is compared against the original XGBoost model from Part 6. The final performance summary table is updated with the new results.\n",
    "\n",
    "-   **Original XGBoost (Test)**: RMSE: 3.84e+10, RÂ²: 0.2835\n",
    "-   **XGBoost + TGN Emb (Test)**: RMSE: 3.74e+10, RÂ²: 0.3197\n",
    "\n",
    "The results clearly demonstrate that the addition of TGN-derived embeddings, as engineered in this simplified GCN-LSTM setup, **successfully improved the performance of the XGBoost model**. The RMSE decreased and the RÂ² increased, suggesting that these learned embeddings captured valuable predictive signals beyond what was available in the handcrafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "part7-final-compare-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Model Performance Summary (Including TGN Augmented Model) ---\n",
      "                           Model          RMSE           MAE        R2\n",
      "0  Naive Forecast (amount_lag_1)  5.664885e+09  1.985531e+09  0.984373\n",
      "1    Historical Average Forecast  1.567316e+10  4.272475e+09  0.880376\n",
      "6       XGBoost + TGN Emb (Test)  3.737605e+10  6.197175e+09  0.319716\n",
      "4                 XGBoost (Test)  3.835765e+10  6.463047e+09  0.283514\n",
      "3           Random Forest (Test)  4.115848e+10  6.662747e+09  0.175060\n",
      "2                LightGBM (Test)  4.578706e+10  9.083452e+09 -0.020914\n",
      "5                    LSTM (Test)  5.227148e+10  1.458294e+10  0.144294\n",
      "\n",
      "Final performance summary for Part 7 saved to processed_for_modeling/model_performance_summary_part7.csv\n"
     ]
    }
   ],
   "source": [
    "df_performance_final_part7 = pd.DataFrame(model_performance_summary_part7)\n",
    "print(\"\\n--- Overall Model Performance Summary (Including TGN Augmented Model) ---\")\n",
    "if not df_performance_final_part7.empty:\n",
    "    df_performance_final_part7.drop_duplicates(subset=['Model'], keep='last', inplace=True)\n",
    "    print(df_performance_final_part7.sort_values(by='RMSE'))\n",
    "    df_performance_final_part7.to_csv(FINAL_MODEL_PERFORMANCE_FILE_PART7, index=False)\n",
    "    print(f\"\\nFinal performance summary for Part 7 saved to {FINAL_MODEL_PERFORMANCE_FILE_PART7}\")\n",
    "else:\n",
    "    print(\"No models were evaluated in Part 7, or summary is empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-end",
   "metadata": {},
   "source": [
    "## End of Part 7\n",
    "\n",
    "This part successfully explored generating dynamic node embeddings using a GCN-LSTM approach and using them to augment the feature set for the best-performing XGBoost model. \n",
    "\n",
    "**Performance Comparison:**\n",
    "- **XGBoost (Test)** (with Part 4 dynamic features): **RMSE: 3.84e+10**, **RÂ²: 0.2835**\n",
    "- **XGBoost + TGN Emb (Test)** (with TGN embeddings): **RMSE: 3.74e+10**, **RÂ²: 0.3197**\n",
    "\n",
    "The TGN-derived embeddings **successfully improved XGBoost performance**, reducing RMSE and increasing R-squared. This result highlights the value of learned graph representations. Even with a simplified GCN-LSTM model and limited computational resources, the TGN-inspired features captured a predictive signal that was complementary to the manually engineered dynamic features from Part 4. This is a powerful validation of the project's core hypothesis and demonstrates the potential of hybrid GNN-ML approaches for complex forecasting tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
